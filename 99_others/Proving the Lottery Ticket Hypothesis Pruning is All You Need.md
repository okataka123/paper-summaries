# Proving the Lottery Ticket Hypothesis: Pruning is All You Need

## Abstract
宝くじ仮説（Frankle and Carbin, 2018）は、ランダムに初期化されたネットワークの中には、小さなサブネットワークが存在し、それを単独で学習させた場合でも、元のネットワークと同等の性能を発揮できると述べている。我々は、さらに強力な仮説（Ramanujanら、2019でも予想されていた）を証明し、あらゆる有界な分布および有界な重みを持つ任意のターゲットネットワークに対して、十分に過剰パラメータ化されたランダム重みのニューラルネットワークには、追加の学習を行うことなく、ターゲットネットワークとほぼ同等の精度を持つサブネットワークが含まれていることを示す。

### 1. Introduction
ニューラルネットワークのプルーニングは、学習済みモデルのサイズを削減し、推論時の計算を効率化しつつ、精度の低下を最小限に抑えるための一般的な手法である。しかし、この手法では依然として、過剰パラメータ化されたネットワークの学習プロセスが必要であり、プルーニングされたネットワークを初めから学習させようとしてもうまくいかないように見える（[10] を参照）。

最近、FrankleとCarbinによる研究 [10] は、驚くべき現象を提示した。プルーニングされたニューラルネットワークは、重みを初期値にリセットすることで、良好な性能を発揮できることが分かった。したがって、著者らは宝くじ仮説を提唱する。すなわち、ランダムに初期化されたニューラルネットワークには、単独で学習させたときに元のネットワークと同等の性能を示すサブネットワークが含まれている、というものである。

この観察結果は大きな関心を呼び、多くの後続研究がこの興味深い現象の解明を試みている。特に、Zhou ら [37] や Ramanujan ら [27] による非常に最近の研究では、学習を一切行うことなく、すでに良好な性能を示すサブネットワークを見つけるアルゴリズムが提示されている。文献 [27] は以下のような予想を立てている。すなわち、ランダムに初期化された、十分に過剰パラメータ化されたニューラルネットワークには、（大規模な学習済みネットワークと比較して）競争力のある精度を達成するサブネットワークが含まれており、それは学習を必要としない。この予想は、宝くじ仮説のより強力なバージョンとみなすことができる。

本研究では、過剰パラメータ化されたニューラルネットワークにおいて、このより強力な予想（強化された宝くじ仮説）を証明する。また、サブネットワークには2種類あることを区別する。すなわち、特定の重みを除去するサブネットワーク（重みサブネットワーク）と、全体のニューロンを除去するサブネットワーク（ニューロンサブネットワーク）である。

まず、任意の深さ$l$のReLUネットワークは、深さ$2l$で十分な幅を持つランダムネットワークの重みサブネットワークを見つけることで近似できることを示す。次に、深さ2（すなわち1つの隠れ層）を持つネットワークには、最良のランダム特徴分類器（すなわち、ネットワークの第2層のみを学習させたときに得られる最良の分類器）と同等の性能を持つニューロンサブネットワークが存在することを示す。したがって、浅いネットワークにおいては、ネットワークの第2層を学習させることは、十分に大きなランダムネットワークからニューロンをプルーニングすることと同等であると示唆される。

我々のすべての結果において、初期ネットワークのサイズは問題のパラメータに対して多項式的な大きさである。重みサブネットワークの場合、プルーニング後のネットワークにおけるパラメータ数は、定数倍の範囲でターゲットネットワークのパラメータ数と同程度であることを示している。

我々の知る限り、本研究はランダムに初期化されたニューラルネットワークの中に良好なサブネットワークが存在するという理論的証拠（すなわち、強化された宝くじ仮説の証明）を初めて示したものである。我々の結果は、本質的に、ランダムに初期化されたネットワークをプルーニングすることが、重みの値を最適化することと同等に強力であることを示唆している。したがって、良いネットワークを見つけるための一般的な手法がパラメータの学習であるのに対し、本研究は、実際には必要なのは優れたプルーニング機構であることを示している。このことは、重みの値を最適化するのではなく、重みのプルーニングに焦点を当てたアルゴリズムの開発に強い動機づけを与える。

### 1.1. Related Work
#### ニューラルネットワークのプルーニング
ニューラルネットワークのプルーニングは、大規模なモデルを圧縮し、リソースが限られたデバイス上での実行を可能にするための一般的な手法である。これまでにさまざまなプルーニング手法が提案されており、最大90%のモデル削減が最小限の性能低下で達成できることが示されている。これらの手法は、「どのようにプルーニングするか（プルーニングの基準）」と「何をプルーニングするか（特定の重み vs. ニューロン全体または畳み込みチャネル）」という2つの観点で異なる。

LeCun ら［16］、Hassibi と Stork［14］、Dong ら［7］による研究では、2次導関数に基づくネットワークプルーニングの効率性が検討されている。もう一つの一般的な手法は、重みの大きさに基づいてプルーニングを行うものである［13］。その他のプルーニング手法としては、ゼロ活性のニューロンを除去するもの［15］や、その他の冗長性指標に基づくもの［22, 32］がある。

重みベースのプルーニングはネットワーク圧縮において最良の結果を示す一方で、推論時間の短縮という点では最適とは言えない。というのも、現代のハードウェアでは効率的に利用することが難しいためである。実際の性能向上を得るために、近年の研究ではニューロン全体や畳み込みチャネルをプルーニングする手法が提案されている［35, 18, 23, 20］。

本研究では、驚くべきことに、ランダムなネットワークをプルーニングすることで、重みを最適化する場合と競争力のある結果が得られることを示す。さらに、ニューロンベースのプルーニングと重みベースのプルーニングを比較し、後者の方が厳密に優れた性能を達成できることを示す。このようなプルーニング手法の強みと限界を理論的に検討した研究は、我々の知る限り存在しない。


#### 宝くじ仮説
文献［10］において、Frankle と Carbin は元の宝くじ仮説を提唱した。すなわち、ランダムに初期化された高密度なニューラルネットワークには、単独で学習させたときに、元のネットワークと同等のテスト精度を（同じ回数以下の学習反復で）達成できるような初期化済みサブネットワークが含まれている、というものである。この仮説が正しければ、非常に有望な実用的示唆を含んでいる。すなわち、大規模ネットワークの非効率な学習プロセスは本来不要であり、良い小規模なサブネットワークを見つけ、それを個別に学習させればよいということになる。良いサブネットワークを見つけることは簡単ではないにせよ、数百万のパラメータを持つニューラルネットワークを学習させるよりは単純かもしれない。

その後の研究である Zhou ら［37］は、「当たりくじ（winning tickets）」、すなわち良い初期サブネットワークは、学習を一切行わなくてもランダムより優れた性能をすでに持っていると主張している。この知見に基づき、彼らはランダムに初期化されたネットワークの中から良好な精度を達成するサブネットワークを見つけ出すアルゴリズムを提案している。この研究を基に、Ramanujan ら［27］は、さまざまなアーキテクチャとデータセットに対して、学習を行わなくても最先端に近い性能を示すサブネットワークを見つけるための改良アルゴリズムを提案した。これらの観察結果を受けて、文献［27］は元の宝くじ仮説に対する補完的な予想を提案している。すなわち、ランダムな重みを持つ十分に過剰パラメータ化されたニューラルネットワーク（例：初期化時）には、競争力のある精度を達成するサブネットワークが存在するというものである。

これらの結果は非常に興味深い主張を含んでいるが、いずれも経験的観察にのみ基づいている。我々の研究は、これらの経験的結果に対して理論的根拠を与えることを目的としている。我々は、深層および浅層ニューラルネットワークの場合において、文献［27］で提示された後者の予想（強い宝くじ仮説）を証明する。我々の知る限り、これは文献［27］で述べられた強い宝くじ仮説を説明しようとする初の理論的研究である。

#### Over-parameterization and random features
近年の有力な研究の一分野では、非常に過剰パラメータ化されたニューラルネットワークに対する勾配法が、さまざまなターゲット関数を多項式時間で学習できることが示されている（例：[2]、[6]、[3]、[5]）。しかしながら、近年の研究（例：[36]、[12]、[11]）では、上記アプローチにおける解析の限界が示されており、その解析の力がランダム特徴のそれと比較されている。特に文献［36］では、このアプローチでは、たとえ分布が標準ガウスであっても、単一のReLUニューロンさえ効率的に近似できないことが示されている。

本研究では、浅いニューロンサブネットワークの探索は、ランダム特徴による学習と同等であることを示す。また、重みサブネットワークは、より弱い仮定（すなわち分布が有界であること）を前提とした場合において、ReLUニューロンを効率的に近似できるという点で、明確により強力なモデルであることを示す。


### 1.2. Notations
以下において使用するいくつかの記法を導入する。インスタンス空間を$\mathcal{X} = \{x \in \mathbb{R}^d : ||x||_2 \leq 1 \}$と表す。$\mathcal{X} \times \mathcal{Y}$上の分布$\mathcal{D}$に対して、仮説$h:\mathcal{X} \rightarrow \mathbb{R}$の二乗損失を次のように表す：

$$
\begin{align}
    L_D(h) = \mathbb{E}_{(x,y) \sim \mathcal{D}}[(h(x)-y)^2]
\end{align}
$$

2つの行列$A, B \in \mathbb{R^{m \times n}}$に対して、Hadamard積（要素ごとの席）を$A \odot B =[A_{i,j}B_{i,j}]_{i,j}$と表す。$U([-c, c]^k)$はゼロを中心とする立方体上の一様分布を表す。$\mathcal{N}(0, \Sigma)$は平均ゼロ、共分散行列$\Sigma$の正規分布を表す。行列$H$に対しては、最小固有値を$\lambda_{\min}(H)$と表す。行列$A$に対しては、$L_2$作用素ノルムを$||A||_2 := \lambda_{\max}(A)$とし、ここで$\lambda_{\max}$は$A$の最大特異値とする。また、行列の$A$の最大ノルム（max norm）は$||A||_{\max} := \max_{i,j}|A_{i,j}|$と表す。


### 2. Approximating ReLU Networks by Pruning Weights

本節では、深さ$l$のネットワークが、深さ$2l$のランダムネットワークをプルーニングすることで近似可能であるという主要な結果を示す。この結果は、特定の重みをプルーニングすることが許され、ニューロン全体の除去（すなわちニューロンサブネットワーク）に限定されない設定、すなわち重みサブネットワークに対して示す。ニューロンサブネットワークについては次節で議論する。

ここでは、活性化関数に ReLU、すなわち$\sigma(x) = \max\{x, 0\}$を用いたネットワークに焦点を当てる。深さ$l$、幅$n$のネットワークで$G:\mathbb{R}^d \rightarrow \mathbb{R}$を次のように定義する：

$$
\begin{align}
    G(x) = G^{(l)} \circ \cdots \circ G^{(1)}(x)
\end{align}
$$

ここで、
- $G^{(1)}(x) = \sigma(W_G^{(1)} x), \quad \text{for } W_G^{(1)} \in \mathbb{R}^{d \times n}$
- $G^{(i)}(x) = \sigma(W_G^{(i)} x), \quad \text{for } W_G^{(i)} \in \mathbb{R}^{n \times n}, \quad \text{for every } 1 < i < l$
- $G^{(l)}(x) = W_G^{(l)} x, \quad \text{for } W_G^{(l)} \in \mathbb{R}^{n \times 1}$
